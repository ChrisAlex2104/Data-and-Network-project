{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d47c8e5",
   "metadata": {},
   "source": [
    "# ECEN 4322-5322 Data and Network Science\n",
    "\n",
    "## Title- Fare prediction for flights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684bae4a",
   "metadata": {},
   "source": [
    "#### Group Members - Chirag Chandrashekar, Chris Alexander, Viveka Salinamakki"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1c6021",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58368eee",
   "metadata": {},
   "source": [
    "The dataset chosen for analysis is the itineraries of flights in the USA over **6 months**. For exploratory data analysis, we aim to find the airport with the highest traffic or the best-connected airports, the price of flights during the different hours of the day, the average/minimum travel distance for which people prefer flight, whether an average flight is fully booked/percentage of flights booked, and popular airlines. The end goal here is to predict the fare of a flight. Due to the large number of rows and features, the prediction can provide a good estimation of the fare. Techniques such as data grouping and manipulation, visualization, regular expressions, data modeling, feature engineering, model validation, and prediction will help achieve our goal. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e85299a",
   "metadata": {},
   "source": [
    "\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30096889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "from ecen5322_utils import run_linear_regression_test\n",
    "\n",
    "# Plot settings\n",
    "plt.rcParams['figure.figsize'] = (12, 9)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3f8245",
   "metadata": {},
   "source": [
    "### Sampling the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4755c538",
   "metadata": {},
   "source": [
    "The origianl data file is sampled due to its size of **30gb** not able to be read on to the ram. Hence, this is solved by reading the file into the chunks of 1million entries and sampling all the chunks to get a final dataset of **500 thousand entries**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c4b82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "dfs=[]\n",
    "with pd.read_csv(\"car_price_prediction.csv\", chunksize=1000000) as reader: #read as chunks due to low system resorces to read 30gb file\n",
    "    reader\n",
    "    for chunk in reader:\n",
    "        #print(type(chunk))\n",
    "        data_index=chunk.index\n",
    "        shuffled_indices = np.random.permutation(data_index)  #shuffling and sampling data to reduce the data to 500000 entries\n",
    "        #print(chunk.loc[shuffled_indices])\n",
    "        #print(type(chunk))\n",
    "        chunk,leftover= np.split(chunk.loc[shuffled_indices],[12000]) #selects first N rows from each chunk\n",
    "        #print(type(chunk))\n",
    "        #print(chunk)\n",
    "        #chunks=chunk.to_frame\n",
    "        #print(chunks)\n",
    "        dfs.append(chunk) #makes a list of dataframe chunks\n",
    "        #joined_chunk=pd.concat(chunk)\n",
    "        #print(joined_chunk)\n",
    "final_df=pd.concat(dfs) #concats all the chunks in the list of dataframe\n",
    "print(\"final dataframe\")\n",
    "final_df\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70e2323",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data to new csv file\n",
    "#final_df\n",
    "'''\n",
    "data_index2=final_df.index\n",
    "shuffled_indices2 = np.random.permutation(data_index2)\n",
    "final_df2,leftover2=np.split(final_df.loc[shuffled_indices2],[500000])\n",
    "final_df2\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f91654b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import  files\n",
    "#final_df2.to_csv('sampled_file.csv')\n",
    "#files.download('sampled_file.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eb6287",
   "metadata": {},
   "source": [
    "Importing the sampled csv data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e4b577",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data=pd.read_csv('sampled_file.csv')\n",
    "\n",
    "#New read_csv function to read empty values as -1 and remove unamed column\n",
    "data=pd.read_csv(\"sampled_file.csv\"\n",
    "                 #, dtype=str\n",
    "                 , keep_default_na=True\n",
    "                 , na_values=-1\n",
    "                 , na_filter=True).drop(columns=['Unnamed: 0'])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0652cd25",
   "metadata": {},
   "source": [
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Cleaning the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0193a94c",
   "metadata": {},
   "source": [
    "### Drop rows with values 'NaN'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24e99fb",
   "metadata": {},
   "source": [
    "The rows with the values **'NaN'** will be dropped except for the description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbcd5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=['totalTravelDistance'])\n",
    "print(data.iloc[:,0:28].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a54c234",
   "metadata": {},
   "source": [
    "### Function to change True and False to 1 and 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079d5029",
   "metadata": {},
   "source": [
    "The columns with values True and False will be changed to 1 and 0 which will be better to train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8d6c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_true_false(df, columnName):\n",
    "    \"\"\"\n",
    "        Inputs:\n",
    "            df [Dataframe]: Dataframe on which the operation is performed\n",
    "            columnName [String]: The column which is being modified\n",
    "        \n",
    "        Output:\n",
    "            New Dataframe with the modified\n",
    "    \"\"\"\n",
    "    df[columnName]=df[columnName].map(dict({True: 1, False: 0}))\n",
    "    return df\n",
    "\n",
    "data=map_true_false(data,'isBasicEconomy')\n",
    "data=map_true_false(data,'isRefundable')\n",
    "data=map_true_false(data,'isNonStop')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a0bd0f",
   "metadata": {},
   "source": [
    "### Function to remove outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe3e76f",
   "metadata": {},
   "source": [
    "`remove_outliers` function removes the outliers depending on the given condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b1fc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(data, variable, lower=-np.inf, upper=np.inf):\n",
    "  \n",
    "    \"\"\"\n",
    "    Input:\n",
    "      data (data frame): the table to be filtered\n",
    "      variable (string): the column with numerical outliers\n",
    "      lower (numeric): observations with values lower than this will be removed\n",
    "      upper (numeric): observations with values higher than this will be removed\n",
    "    \n",
    "    Output:\n",
    "      a data frame with outliers removed\n",
    "      \n",
    "    Note: This function should not change mutate the contents of data.\n",
    "    \"\"\"  \n",
    "    data_2=data[data[variable]<= upper]\n",
    "    data_3=data_2[data_2[variable]>= lower]\n",
    "    return data_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc43494",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = remove_outliers(data, 'baseFare', lower = 60)\n",
    "data.loc[:,'baseFare'].sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfd4597",
   "metadata": {},
   "source": [
    "### rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae3418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ccf832",
   "metadata": {},
   "source": [
    "### Convert date time column to date time format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f393c0b3",
   "metadata": {},
   "source": [
    "The dates are converted to datetime format using `convertToDateTimeFormat` so that the year, month and day will be easily accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0391b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToDateTimeFormat(df, columnName):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        df [Dataframe]: Dataframe on which the operation is performed\n",
    "        columnName [String]: The column which has dates to be converted into datetime format\n",
    "                        Acceptable names are 'searchDate' and 'flightDate'\n",
    "        \n",
    "    Output:\n",
    "        New Dataframe with dates converted into datetime format\n",
    "    \"\"\"\n",
    "    df[columnName]=pd.to_datetime(df[columnName], format=\"%Y-%m-%d\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb20be81",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=convertToDateTimeFormat(data, 'searchDate')\n",
    "data=convertToDateTimeFormat(data, 'flightDate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a98856",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['searchDate']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0ffda4",
   "metadata": {},
   "source": [
    "### Create columns for month and day for the date columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ceaf9c",
   "metadata": {},
   "source": [
    "The month and year are extracted from the date columns using `createMonthDay` so that they more usable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c22d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMonthDay(df, columnName):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        df [Dataframe]: Dataframe on which the operation is performed\n",
    "        columnName [String]: The column which has dates from which month and year are extracted\n",
    "                        Acceptable names are 'searchDate' and 'flightDate'\n",
    "        \n",
    "    Output:\n",
    "        New Dataframe with with the columns for month and year added from the the column columnName\n",
    "    \"\"\"\n",
    "    df[columnName+'_month']=df[columnName].dt.month\n",
    "    df[columnName+'_day']=df[columnName].dt.day\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2879d1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=createMonthDay(data, 'searchDate')\n",
    "data=createMonthDay(data, 'flightDate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e420439",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2086c340",
   "metadata": {},
   "source": [
    "### Extracting duration of the flight in hours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea70443",
   "metadata": {},
   "source": [
    "Current format of the time is in as string form with other character, Eg: `PT5H17M` which is 5 hours and 17 mins. the time is extracted using regex and saved into a new `Flight_duration` column in hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14e3bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_travel_duration(dataFrame, columnName):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        df [Dataframe]: Dataframe on which the operation is performed\n",
    "        columnName [String]: The column which is being modified\n",
    "        \n",
    "    Output:\n",
    "        New Dataframe with the modified\n",
    "    \"\"\"\n",
    "    time_columns = pd.DataFrame()\n",
    "    hour_segment=r\"(\\d+)H\"\n",
    "    min_segment=r\"(\\d+)M\"\n",
    "    time_columns[\"Hour\"]=dataFrame[columnName].str.extract(hour_segment).fillna(0).astype(int)\n",
    "    time_columns[\"Min\"]=dataFrame[columnName].str.extract(min_segment).fillna(0).astype(int)\n",
    "    dataFrame[\"Flight_duration\"] = time_columns[\"Hour\"]+(time_columns[\"Min\"]/60)\n",
    "    return dataFrame\n",
    "    \n",
    "data=extract_travel_duration(data, \"travelDuration\")\n",
    "data[\"Flight_duration\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe6b1cc",
   "metadata": {},
   "source": [
    "### Extract segments using regex "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a482d2b3",
   "metadata": {},
   "source": [
    "The data has many columns with data about different airports, airlines, etc., and the flights with layovers have all these data in the same column. We are using regular expressions to extract the different segments and separate them into different columns.\n",
    "\n",
    "For example: The airport codes `ATL||JFK` will be split into `ATL` and `JFK` and added under different columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05504369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractSegments(dataFrame, columnName):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        df [Dataframe]: Dataframe on which the operation is performed\n",
    "        columnName [String]: The column which is being modified\n",
    "        \n",
    "    Output:\n",
    "        New Dataframe with the modified\n",
    "    \"\"\"\n",
    "    #Pattern for seperating the segments\n",
    "    segmentPattern=r\"([^||]+)\"\n",
    "   \n",
    "    #Create new columns for the extracted segments\n",
    "    dataFrame[columnName+\"1\"]=dataFrame[columnName].str.findall(segmentPattern).str[0]\n",
    "    dataFrame[columnName+\"2\"]=dataFrame[columnName].str.findall(segmentPattern).str[1]\n",
    "    dataFrame[columnName+\"3\"]=dataFrame[columnName].str.findall(segmentPattern).str[2]\n",
    "    return dataFrame\n",
    "\n",
    "columnsWithSegments=[iterator for iterator in data if iterator.startswith('segments')]\n",
    "\n",
    "for i in columnsWithSegments:\n",
    "    data=extractSegments(data, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a07a91",
   "metadata": {},
   "source": [
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58e53e5",
   "metadata": {},
   "source": [
    "### Distribution of Base Fare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e46488",
   "metadata": {},
   "source": [
    "We plot a histogram for base fare to see its distribution. This distribution helps us understand how the values are spread apart and helps find outliers present in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94947167",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data['baseFare'],color=\"blue\",kde=True,label='baseFare')\n",
    "plt.title(\"Histogram of BasePrice column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3749e37a",
   "metadata": {},
   "source": [
    "From the plot it seems like most of the entries base price are in the range of 0-1500. values above 1500 are not outliers but  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010d12e0",
   "metadata": {},
   "source": [
    "### Best Connected Airport (Airport with the highest traffic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936e1215",
   "metadata": {},
   "source": [
    "The best connected data will be the one which has the highest amount of traffic passing through it. The best connected airport will be found out using the arrival and departure data given in the dataframe.\n",
    "\n",
    "The layovers will have redundant data i.e., the airport will be repeated in the arrival and departure columns which will be taken into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b436762",
   "metadata": {},
   "outputs": [],
   "source": [
    "airportCodesSeries=data['segmentsArrivalAirportCode1'].append(data['segmentsArrivalAirportCode2']).append(data['segmentsArrivalAirportCode3']).append(data['segmentsDepartureAirportCode1'])\n",
    "airportCodesSeries.value_counts().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a890954",
   "metadata": {},
   "outputs": [],
   "source": [
    "Height_plot=np.array(airportCodesSeries.value_counts().head(5))\n",
    "plt.bar(x=airportCodesSeries.value_counts().head(5).index,height=Height_plot,color=['orange', 'red', 'green', 'purple', 'pink'])\n",
    "plt.title(\"Top 5 best connected airports\")\n",
    "plt.xlabel(\"Airports\")\n",
    "plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c446616",
   "metadata": {},
   "source": [
    "The **Chicago, IL Oâ€™Hare (ORD)** airport is the best connected airport meaning that the highest amount of domestic air traffic goes through this airport compared to other airports in the United States."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b57ac7",
   "metadata": {},
   "source": [
    "### Fare price for different flight duration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2133f9c4",
   "metadata": {},
   "source": [
    "Plotting a graph of Fare price vs flight duration will give an understanding of its trend and any correlation which might be present between the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cf7248",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=data[\"Flight_duration\"], y=data[\"baseFare\"])\n",
    "plt.title(\"Basefare vs Flight duration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2188d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#condition=data[data[\"Flight_duration\"]>=2.5 and data[\"Flight_duration\"]<=12.5]\n",
    "#sns.lineplot(x=condition[\"Flight_duration\"], y=data[\"baseFare\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366094f0",
   "metadata": {},
   "source": [
    "From the above plot there seems to be a linear relationship between flight duration and baseFare in the range 2 to 15 hour flights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c5fcf1",
   "metadata": {},
   "source": [
    "### The minimum and average distance for which people prefer flights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7926f874",
   "metadata": {},
   "source": [
    "The distance travelled in each travel is calculated using segmentDistance columns and so the distance for which people prefer flights can be analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34052a1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.loc[:,['segmentsDistance','segmentsDistance1', 'segmentsDistance2', 'segmentsDistance3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b9ec60",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['segmentsDistance1']=data['segmentsDistance1'].fillna(0).astype(int)\n",
    "data['segmentsDistance2']=data['segmentsDistance2'].fillna(0).astype(int)\n",
    "data['segmentsDistance3']=data['segmentsDistance3'].fillna(0).astype(int)\n",
    "data['totalDistance']=data['segmentsDistance1']+data['segmentsDistance2']+data['segmentsDistance3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76086ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['totalDistance'].describe(), data['totalDistance'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d650136",
   "metadata": {},
   "source": [
    "The minimum distance travelled using domestic flights is **89 miles** and a median distance covered by a domestic flight is **1482 miles**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73c28f5",
   "metadata": {},
   "source": [
    "### whether the average flight is fully booked /average% of seat booked- Chirag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdfec04",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSegment1 = data.groupby('segmentsAirlineName1', as_index=False)['seatsRemaining'].sum()\n",
    "dataSegment2 = data.groupby('segmentsAirlineName2', as_index=False)['seatsRemaining'].sum()\n",
    "dataSegment3 = data.groupby('segmentsAirlineName3', as_index=False)['seatsRemaining'].sum()\n",
    "dataSegment1['averageSeatsRemaining'] = (dataSegment1['seatsRemaining']/len(data.index)).round(0)\n",
    "print(dataSegment1.sort_values(by = 'seatsRemaining', ascending = False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f7c78f",
   "metadata": {},
   "source": [
    "### top 5 most popular airlines- Chirag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b2f1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "airlineName=data['segmentsAirlineName1'].append(data['segmentsAirlineName2']).append(data['segmentsAirlineName3']).append(data['segmentsAirlineName1'])\n",
    "airlineName.value_counts().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483db563",
   "metadata": {},
   "source": [
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Prediction of fare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a542ee",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a799845",
   "metadata": {},
   "source": [
    "### One hot encoding for categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2b5c89",
   "metadata": {},
   "source": [
    "The categorical data present cannot be used directly in the prediction as they do not contain measurable data. The data is hence converted into features using `OneHotEncoder` from `sklearn`.\n",
    "\n",
    "The columns identified to be OneHotEncoded are:\n",
    "`startingAirport`, `destinationAirport`, `segmentsArrivalAirportCode`, `segmentsDepartureAirportCode`, `segmentsAirlineName`, `segmentsAirlineCode`, `segmentsCabinCode`.\n",
    "\n",
    "In these columns, `segmentsAirlineName` and `segmentsAirlineCode` convey the same information and therefore redundant. One of the 2 columns is suffiecient and we have chosen `segmentsAirlineCode`.\n",
    "\n",
    "The data of `startingAirport` and `destinationAirport` are already included in `segmentsArrivalAirportCode` and `segmentsDepartureAirportCode` and so are redundant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eb89b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc89758b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oheCabinCodeColumns(oheData):\n",
    "    \"\"\"\n",
    "        Inputs:\n",
    "            oheData [Dataframe]: Dataframe on which the operation is performed\n",
    "        \n",
    "        Output:\n",
    "            Dataframe with the OneHotEncoded features from the column 'segmentsCabinCode'\n",
    "    \"\"\"\n",
    "    \n",
    "    #columnName is the column to be OneHotEncoded\n",
    "    columnName='segmentsCabinCode'\n",
    "    \n",
    "    #The column 'segmentsCabinCode' has been split into 3 different columns (the layover data) and so all the 3 columns have to be OneHotEncoded\n",
    "    oneHotEnc = OneHotEncoder(dtype=int, handle_unknown='ignore')\n",
    "    oheDataColumn3 = oneHotEnc.fit_transform(oheData[[columnName+'3']]).toarray()\n",
    "    numberOfColumns3=np.shape(oheDataColumn3)[1]\n",
    "    oheDataColumn2 = oneHotEnc.fit_transform(oheData[[columnName+'2']]).toarray()\n",
    "    numberOfColumns2=np.shape(oheDataColumn2)[1]\n",
    "    oheDataColumn1 = oneHotEnc.fit_transform(oheData[[columnName+'1']]).toarray()\n",
    "    numberOfColumns1=np.shape(oheDataColumn1)[1]\n",
    "    \n",
    "    #The minimum number of columns are selected because the segment2 and segment3 have NaNs which are absent in segment1 and also unnecessary to the prediction\n",
    "    numberOfColumns=min(numberOfColumns3, numberOfColumns2, numberOfColumns1)\n",
    "    \n",
    "    #The OR operation is done as the features from the 3 columns are the data of the same categories and so a common set of columns for all the 3 segments will suffice\n",
    "    oheData[oneHotEnc.categories_[0]] = oheDataColumn1[:,:numberOfColumns] | oheDataColumn2[:,:numberOfColumns] | oheDataColumn3[:,:numberOfColumns]\n",
    "    return oheData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d36c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data=oheCabinCodeColumns(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6603d8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oheSegmentsAirportCode_AirlineName(oheData, columnName):\n",
    "    \"\"\"\n",
    "        Inputs:\n",
    "            oheData [Dataframe]: Dataframe on which the operation is performed\n",
    "            columnName [String]: The column name (only the ones mentioned below) of the column to be OneHotEncoded\n",
    "                                 Acceptable columnNames are 'segmentsArrivalAirportCode', 'segmentsDepartureAirportCode', or 'segmentsAirlineCode'\n",
    "        \n",
    "        Output:\n",
    "            Dataframe with the OneHotEncoded features from the column columnName\n",
    "    \"\"\"\n",
    "\n",
    "    oneHotEnc = OneHotEncoder(dtype=int, handle_unknown='ignore')\n",
    "    \n",
    "    #The acceptable columnName columns have been split into 3 different columns (the layover data) and so all the 3 columns have to be OneHotEncoded\n",
    "    #dfIntermediate dataframes are to get the features of each of the segments individually\n",
    "    dfIntermediate3=pd.DataFrame()\n",
    "    dfIntermediate2=pd.DataFrame()\n",
    "    dfIntermediate1=pd.DataFrame()\n",
    "    oheDataColumn3 = oneHotEnc.fit_transform(oheData[[columnName+'3']]).toarray()\n",
    "    dfIntermediate3[oneHotEnc.categories_[0]]=oheDataColumn3\n",
    "    oheDataColumn2 = oneHotEnc.fit_transform(oheData[[columnName+'2']]).toarray()\n",
    "    dfIntermediate2[oneHotEnc.categories_[0]]=oheDataColumn2\n",
    "    oheDataColumn1 = oneHotEnc.fit_transform(oheData[[columnName+'1']]).toarray()\n",
    "    dfIntermediate1[oneHotEnc.categories_[0]]=oheDataColumn1\n",
    "    \n",
    "    #The OR operation is done as the features from the 3 columns are the data of the same categories and so a common set of columns for all the 3 segments will suffice\n",
    "    dfIntermediate = (dfIntermediate3 | dfIntermediate2 | dfIntermediate1)\n",
    "    \n",
    "    #Dropping the feature NaN\n",
    "    dfIntermediate = dfIntermediate.loc[:, dfIntermediate.columns.notna()]\n",
    "    \n",
    "    #Adding columnName as the prefix to the features to differentiate between the arrival and departure data\n",
    "    dfIntermediate.columns=[columnName+'_'+iterator for iterator in dfIntermediate.columns]\n",
    "    \n",
    "    oheData[dfIntermediate.columns] = dfIntermediate\n",
    "    \n",
    "    #Replacing NaNs with 0\n",
    "    oheData[dfIntermediate.columns] = oheData[dfIntermediate.columns].fillna(0).astype(int)\n",
    "    return oheData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900cbbb6",
   "metadata": {},
   "source": [
    "The categorical data have been coverted into features to be used in the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293e73ed",
   "metadata": {},
   "source": [
    "### pipeline function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5dd033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_gm(data, pipeline_functions):\n",
    "    \"\"\"Process the data for a guided model.\"\"\"\n",
    "    for function, arguments, keyword_arguments in pipeline_functions:\n",
    "        if keyword_arguments and (not arguments):\n",
    "            data = data.pipe(function, **keyword_arguments)\n",
    "        elif (not keyword_arguments) and (arguments):\n",
    "            data = data.pipe(function, *arguments)\n",
    "        else:\n",
    "            data = data.pipe(function)\n",
    "    return data\n",
    "\n",
    "def drop_columns(data, *columns):\n",
    "    \"\"\"Drop columns passed as arguments.\"\"\"\n",
    "    return data.drop(columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae025767",
   "metadata": {},
   "outputs": [],
   "source": [
    "#processing the data on Feature engineering functions\n",
    "pipelines = [\n",
    "    (oheCabinCodeColumns, None, None),\n",
    "    (oheSegmentsAirportCode_AirlineName, None, {'columnName':'segmentsArrivalAirportCode'}),    \n",
    "    (oheSegmentsAirportCode_AirlineName, None, {'columnName':'segmentsDepartureAirportCode'}),    \n",
    "    (oheSegmentsAirportCode_AirlineName, None, {'columnName':'segmentsAirlineCode'}),    \n",
    "    (drop_columns, ['Log Sale Price', 'Bedrooms'], None)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffc48a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = process_data_gm(data, pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d445d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e443ad",
   "metadata": {},
   "source": [
    "### Splitting data for testing, training and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6714e6c",
   "metadata": {},
   "source": [
    "We are splitting the data into three segments for testing, training and validation in the proportion of 8:1:1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e364a776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Let's say we want to split the data in 80:10:10 for train:valid:test dataset\n",
    "train_size=0.8\n",
    "\n",
    "X = data.drop(columns = ['baseFare']).copy()\n",
    "y = data['baseFare']\n",
    "\n",
    "# In the first step we will split the data in training and remaining dataset\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(X,y, train_size=0.8)\n",
    "\n",
    "# Now since we want the valid and test size to be equal (10% each of overall data). \n",
    "# we have to define valid_size=0.5 (that is 50% of remaining data)\n",
    "test_size = 0.5\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.5)\n",
    "print(X_train.shape), print(y_train.shape)\n",
    "print(X_valid.shape), print(y_valid.shape)\n",
    "print(X_test.shape), print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f330f88d",
   "metadata": {},
   "source": [
    "### Prediction using Linear Regression without L2 Regularziation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef53259",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model as lm\n",
    "\n",
    "linear_model = lm.LinearRegression(fit_intercept=True) # creating the linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d603de",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_fitted = linear_model.fit(X_train, y_train) #fitting the model on x_train, y_train\n",
    "y_predicted_test = linear_model.predict(X_test) # prediction using x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30a8c93",
   "metadata": {},
   "source": [
    "### Prediction with L2 Regularziation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f4355d",
   "metadata": {},
   "source": [
    "In this model we will be adding Regulariziation to combat overfitting if present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e49c369",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge_model = Ridge(alpha = 10000) # creating the Regularized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c602d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_fitted_l2= ridge_model.fit(X_train, y_train) #fitting the model on x_train, y_train\n",
    "y_predicted_l2_test = ridge_model.predict(X_test_m1)# prediction using x_train\n",
    "lasso_model.corf_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a6e146",
   "metadata": {},
   "source": [
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Validation of model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddf9670",
   "metadata": {},
   "source": [
    "### RMSE Value-Chirag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a1d702",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8dda889e",
   "metadata": {},
   "source": [
    "### plot of the model predictions versus the observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7076ba73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7f4e3b9",
   "metadata": {},
   "source": [
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Inference and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e74408",
   "metadata": {},
   "source": [
    "#### EDA Inference\n",
    "\n",
    "For exploratory data analysis, we aim to find the airport with the highest traffic or the best-connected airports, the price of flights during the different hours of the day, the average/minimum travel distance for which people prefer flight, whether an average flight is fully booked/percentage of flights booked, and popular airlines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf535d9",
   "metadata": {},
   "source": [
    "#### Prediction Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080cda8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
